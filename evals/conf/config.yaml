hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${exp_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO

defaults:
  - prompt: zero-shot
  - language_model: gpt-3.5-turbo

# Common Params
exp_dir: ???
swap: false
seed: 0
limit: 50
reset: false
logging: INFO
print_prompt_and_response: false
cache_dir: ${exp_dir}/cache
prompt_history_dir: ${exp_dir}/prompt_history

# Fine-tuning
dataset: mmlu  # or alpaca_gpt4
num_samples: 20000  # for alpaca_gpt4
topics: ["high_school_mathematics"]  # for mmlu
num_per_topic: 25  # for mmlu
encoding_scheme: identity  # or walnut53

# API
organization: ACEDEMICNYUPEREZ_ORG
anthropic_tag: ANTHROPIC_API_KEY
openai_tag: OPENAI_API_KEY
anthropic_num_threads: 20
openai_fraction_rate_limit: 0.9
